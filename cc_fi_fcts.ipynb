{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e69efee7",
   "metadata": {},
   "source": [
    "### Objective\n",
    "### This file contains functions employed by the principal files \n",
    "### cc_fi_data_preparation_1_1.ipynb ,cc_fi_data_preparation_1_2.ipynb\n",
    "### cc_fi_1_1.ipynb, cc_fi_1_2.ipynb "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c779c067",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "import category_encoders as ce\n",
    "\n",
    "from sklearn.model_selection import  GridSearchCV\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f9ad1c8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dde8a62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function vectorizing a binary column\n",
    "#it returns the dataframe with the column to vectorize\n",
    "#and the vectorized column\n",
    "\n",
    "#val_name_column_to_vectorize=the name of the column to vectorize\n",
    "#val_name_vecrorized_column=the name fo the vecrorized column\n",
    "def fct_vectorize_binary_column(\\\n",
    "val_name_dataframe,\\\n",
    "val_name_column_to_vectorize,\\\n",
    "val_name_vectorized_column):\n",
    "                                \n",
    "    #from sklearn.preprocessing import LabelEncoder\n",
    "    labelencoder = LabelEncoder()\n",
    "\n",
    "    #transform the categorical target variable y to numeric one\n",
    "    val_name_dataframe[val_name_vectorized_column] =\\\n",
    "    labelencoder.fit_transform(val_name_dataframe[val_name_column_to_vectorize])\n",
    "\n",
    "    return val_name_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39555816",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9c67847",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function  vectorizing the all variables (numeric+categorical ones)\n",
    "#of a dataframe and returns a datafram with numeric values\n",
    "#it employs a target based encoder so as to keep the \n",
    "#number of columns unchanged contrary to OneHotEncoder\n",
    "#it returns the vectoirzed dataframehanged contrary to OneHotEncoder\n",
    "\n",
    "#val_name_target_with_categorical_values=the name of the target variable\n",
    "#containing catagorical values \n",
    "\n",
    "def fct_target_vectorization_obj_cols_df(\\\n",
    "val_dataframe,\\\n",
    "val_name_vectorized_target_variable=\"y1\",\n",
    "val_name_non_vectorized_target=\"y\"):\n",
    "    \n",
    "\n",
    "    #the list with the object type columns to vectorize                                            \n",
    "    li_obj_cols=list(val_dataframe.columns)\n",
    "    \n",
    "    #this datframe contains the initial target with the categorical values\n",
    "    #and the vectorized target\n",
    "    #we remove the categorical target variable\n",
    "    li_obj_cols.remove(val_name_non_vectorized_target)\n",
    "                                                 \n",
    "    encoder = ce.LeaveOneOutEncoder(return_df=True,handle_missing='error')\n",
    "\n",
    "    val_dataframe_1=encoder.fit_transform(val_dataframe[li_obj_cols],\\\n",
    "    val_dataframe[val_name_vectorized_target_variable])\n",
    "    \n",
    "    \n",
    "    #we add the target column in the datafrale to return \n",
    "    val_dataframe_1[val_name_non_vectorized_target]=\\\n",
    "    val_dataframe[val_name_non_vectorized_target]\n",
    "    \n",
    "    # we return the dataframe\n",
    "    return val_dataframe_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "03de5801",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#function  vectorizing the categorical variables\n",
    "#of a dataframe and returns a datafram with numeric values\n",
    "#it employs a target based encoder so as to keep the \n",
    "#number of columns unchanged contrary to OneHotEncoder\n",
    "#it returns the vectorzed dataframe\n",
    "\n",
    "#val_name_target_with_categorical_values=the name of the target variable\n",
    "#containing catagorical values \n",
    "\n",
    "def fct_target_vectorization_obj_cols_df_1(\\\n",
    "val_dataframe,\\\n",
    "val_name_vectorized_target_variable=\"y1\",\n",
    "val_name_non_vectorized_target=\"y\"):\n",
    "    \n",
    "    li_numeric_features=val_dataframe.select_dtypes([np.number]).columns\n",
    "    \n",
    "    li_obj_cols =list\\\n",
    "    (val_dataframe.select_dtypes(exclude=[np.number]).columns)\n",
    "    \n",
    "    #the list with the object type columns to vectorize                                            \n",
    "    #li_obj_cols=list(val_dataframe.columns)\n",
    "    \n",
    "    #this datframe contains the initial target with the categorical values\n",
    "    #and the vectorized target\n",
    "    #we remove the categorical target variable\n",
    "    li_obj_cols.remove(val_name_non_vectorized_target)\n",
    "                                                 \n",
    "    encoder = ce.LeaveOneOutEncoder(return_df=True,handle_missing='error')\n",
    "\n",
    "    val_dataframe_1=encoder.fit_transform(val_dataframe[li_obj_cols],\\\n",
    "    val_dataframe[val_name_vectorized_target_variable])\n",
    "    \n",
    "    \n",
    "    #we add the target variable in the datafrale to return\n",
    "    val_dataframe_1[val_name_non_vectorized_target]=\\\n",
    "    val_dataframe[val_name_non_vectorized_target]\n",
    "    \n",
    "    #we add the numeric features in the dataframe to return \n",
    "    val_dataframe_1[li_numeric_features]=val_dataframe[li_numeric_features]\n",
    "    \n",
    "    #we return the dataframe\n",
    "    return val_dataframe_1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04c2246c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8bbd9909",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import math\n",
    "\n",
    "#fct ploting feature variables of a dataframe\n",
    "\n",
    "#data=the data\n",
    "#feature_keys=the feature keys\n",
    "#titles=the titles for each subplot\n",
    "#colors=the list with the colo names for the plots\n",
    "#v_cols=the number of columns for each subplot\n",
    "def fct_show_raw_visualization(\n",
    "    data,\n",
    "    feature_keys,\n",
    "    titles,\n",
    "    colors,\n",
    "    v_cols,\n",
    "    v_name_dataframe):\n",
    "    \n",
    "    #print(\"data.shape[0]\",data.shape[0])\n",
    "    #print()\n",
    "    #print(\"range(len(feature_keys))\",range(len(feature_keys)))\n",
    "    #print()\n",
    "    v_nb_rows=math.ceil(len(feature_keys)/v_cols)\n",
    "    \n",
    "    \n",
    "    #time_data = data[date_time_key]\n",
    "    x_axis_labels=list(range(0,data.shape[0]))\n",
    "    fig, axes = plt.subplots(\n",
    "        nrows=v_nb_rows, ncols=v_cols, figsize=(15, 20), dpi=80, facecolor=\"w\", edgecolor=\"k\"\n",
    "    )\n",
    "    \n",
    "    #title \n",
    "    fig.suptitle('Features dataframe '+str(v_name_dataframe))\n",
    "    \n",
    "    # Remove the subplot at position (9,2)which is element (8,1)\n",
    "    #as numeration starts from zero\n",
    "    if len(feature_keys)<v_nb_rows*v_cols:\n",
    "        row_id=int(v_nb_rows-1)\n",
    "        fig.delaxes(axes[row_id,1])\n",
    "    \n",
    "    \n",
    "    for i in range(len(feature_keys)):\n",
    "        #print(\"i\",i)\n",
    "    \n",
    "        key = feature_keys[i]\n",
    "        \n",
    "        #print(\"key\",key)\n",
    "        c = colors[i]\n",
    "        t_data = data[key]\n",
    "        \n",
    "        #print(\"t_data\",t_data)\n",
    "       \n",
    "        ax = t_data.plot(\n",
    "            ax=axes[i//2, i%2],\n",
    "            color=c,\n",
    "            title=\"{} - {}\".format(titles[i], key),\n",
    "            rot=25,\n",
    "        )\n",
    "        ax.legend([titles[i]])\n",
    "    plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31f799ee",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "817b5260",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function creating a heatmap plot from a dataframe\n",
    "\n",
    "#data=the dataset\n",
    "#val_name_dataframe=the name of the dataframe to add in the title\n",
    "def fct_show_feature_heatmap(data,val_name_dataframe):\n",
    "    plt.matshow(data.corr())\n",
    "    plt.xticks(range(data.shape[1]), data.columns, fontsize=12, rotation=90,color=\"b\")\n",
    "    plt.gca().xaxis.tick_bottom()\n",
    "    plt.yticks(range(data.shape[1]), data.columns, fontsize=12,color=\"b\")\n",
    "\n",
    "    cb = plt.colorbar()\n",
    "    cb.ax.tick_params(labelsize=12)\n",
    "    plt.title(\"Feature Correlation Heatmap-\"+str(val_name_dataframe), fontsize=10,color=\"b\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fab299a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "18d9cfad",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct creating a correlation coefficient matrix from a dataframe\n",
    "\n",
    "#data=the dataset\n",
    "#val_name_dataframe=the name of the dataframe to add in the title\n",
    "def fct_show_cor_coef_matrix(val_data,val_name_dataframe):\n",
    "    \n",
    "    plt.figure(figsize=(14,7))\n",
    "    #plt.figure(figsize=(20,15))\n",
    "\n",
    "    # Create a custom divergin palette\n",
    "    cmap = sns.diverging_palette(100, 7, s=75, l=40,n=5, center=\"light\", as_cmap=True)\n",
    "    \n",
    "    #Create a mask\n",
    "    mask = np.triu(np.ones_like(val_data.corr(), dtype=bool))\n",
    "    np.fill_diagonal(mask, False)\n",
    "    \n",
    "    #scale all fonts in your legend and on the axes.\n",
    "    sns.set(font_scale=1)\n",
    "    \n",
    "    heatmap = sns.heatmap(val_data.corr(), vmin=-1, vmax=1, annot=True,\\\n",
    "                          annot_kws={'fontsize': 8},cmap=\"PiYG\", mask=mask)\n",
    "    \n",
    "    heatmap.set_title('Correlation coeffcient matrix-'+str(val_name_dataframe),\\\n",
    "                  fontdict={'fontsize':15}, pad=12,c=\"darkgreen\")\n",
    "    \n",
    "    plt.yticks(rotation=30,fontsize=8,c=\"darkgreen\")\n",
    "    plt.xticks(rotation=20,fontsize=8,c=\"darkgreen\")\n",
    "    \n",
    "    #plt.savefig(\"Figures/fig_cor_heatmap.png\")\n",
    "    #plt.close(fig)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "092fa946",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ba2e901d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function ploting the feature importance\n",
    "\n",
    "#importance= the feature importance\n",
    "#names=the coolumn names\n",
    "#model_type=the algorithm employed (e.g. Random Forest)\n",
    "\n",
    "def plot_feature_importance(importance,names,model_type):\n",
    "\n",
    "    #Create arrays from feature importance and feature names\n",
    "    feature_importance = np.array(importance)\n",
    "    feature_names = np.array(names)\n",
    "\n",
    "    #Create a DataFrame using a Dictionary\n",
    "    data={'feature_names':feature_names,'feature_importance':feature_importance}\n",
    "    fi_df = pd.DataFrame(data)\n",
    "\n",
    "    #Sort the DataFrame in order decreasing feature importance\n",
    "    fi_df.sort_values(by=['feature_importance'], ascending=False,inplace=True)\n",
    "\n",
    "    #Define size of bar plot\n",
    "    plt.figure(figsize=(10,8))\n",
    "    #Plot Searborn bar chart\n",
    "    sns.barplot(x=fi_df['feature_importance'], y=fi_df['feature_names'])\n",
    "    #Add chart labels\n",
    "    plt.title(model_type + ' FEATURE IMPORTANCE',c='b')\n",
    "    plt.xlabel('FEATURE IMPORTANCE',c='b')\n",
    "    plt.ylabel('FEATURE NAMES',c='b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "957b66f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a70fc371",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function  doing the gridsearchcv operation for a desiring score\n",
    "#it returns the gridseaerch object, the best score and\n",
    "#the best model steps\n",
    "\n",
    "#val_pipeline=list with the pipelines\n",
    "#val_params=list with the parameters of the pipelines\n",
    "#val_cv=the number of folders for the k cross validation\n",
    "#val_scoring_metric=the score to use in the gridsearch e.g. \"recall\"\n",
    "#val_X=the dataset\n",
    "#val_y=the target \n",
    "#val_X_test=the test set (we consider \n",
    "#that there is training and test sets only not validation or development test)\n",
    "#val_y_test=he target for the test set\n",
    "\n",
    "def fct_gridsearchcv_single_metric(\\\n",
    "val_pipeline,\\\n",
    "val_params,\\\n",
    "val_cv,\\\n",
    "val_scoring_metric,\\\n",
    "val_X,\\\n",
    "val_y,\\\n",
    "val_X_test,\\\n",
    "val_y_test):\n",
    "    \n",
    "    #define the gridsearch object\n",
    "    gs_obj=\\\n",
    "    GridSearchCV(val_pipeline, val_params, cv=val_cv,scoring=val_scoring_metric)\n",
    "    \n",
    "    #fit the gridsearch object\n",
    "    gs_obj.fit(val_X,val_y)\n",
    "    \n",
    "    #best score\n",
    "    best_score = gs_obj.score(val_X_test, val_y_test)\n",
    "    \n",
    "    #print(\"gs_obj.best_estimator_\",gs_obj.best_estimator_)\n",
    "    \n",
    "    #The best model \n",
    "    #best_model=gs_obj.best_estimator_\n",
    "    \n",
    "    return gs_obj, best_score#, best_model.steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f38aaa55",
   "metadata": {},
   "outputs": [],
   "source": [
    "#function returning a dictionary with the gridsearch information\n",
    "#for different  score metrics\n",
    "\n",
    "#it returns a dict, key=id metric\n",
    "#value=[gridsearch object,best_score]\n",
    "\n",
    "\n",
    "#val_li_score_metrics=list witht he evaluation metrics ot be considered\n",
    "#val_pipeline=the list with the pipelines\n",
    "#val_params=the list with the parameters for each pipeline\n",
    "#val_cv=the number of folders for the k cross validation\n",
    "#val_scoring_metric=the score to use in the gridsearch e.g. \"recall\"\n",
    "#val_X=the dataset\n",
    "#val_y=the target \n",
    "#val_X_test=the test set (we consider \n",
    "#that there is training and test sets only not validation or development test)\n",
    "#val_y_test=he target for the test set\n",
    "\n",
    "def fct_di_results_gridsearchcv_various_score_metrics(\n",
    "val_li_score_metrics,\\\n",
    "val_pipeline,\\\n",
    "val_params,\\\n",
    "val_cv,\\\n",
    "val_X,\\\n",
    "val_y,\\\n",
    "val_X_test,\\\n",
    "val_y_test,\n",
    "):\n",
    "    \n",
    "    #di_Rep=dict, key=id metric\n",
    "    #value=[gridsearch object,best_score, best_model.steps ]\n",
    "    di_rep={}\n",
    "    \n",
    "    #for each metric a gridesearch is operated\n",
    "    for i in val_li_score_metrics:\n",
    "        \n",
    "        #print(\"metric i\",i)\n",
    "        #print()\n",
    "        \n",
    "        gs_obj, best_score=\\\n",
    "        fct_gridsearchcv_single_metric(\\\n",
    "        val_pipeline=val_pipeline,\\\n",
    "        val_params=val_params,\\\n",
    "        val_cv=val_cv,\\\n",
    "        val_scoring_metric=i,\\\n",
    "        val_X=val_X,\\\n",
    "        val_y=val_y,\\\n",
    "        val_X_test=val_X_test,\\\n",
    "        val_y_test=val_y_test)\n",
    "        \n",
    "        di_rep[i]=[gs_obj, best_score]\n",
    "        \n",
    "    return di_rep\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab540fa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3ff4bc39",
   "metadata": {},
   "outputs": [],
   "source": [
    "#fct returning a dictionary, key=id metric\n",
    "#value=[score, best model]\n",
    "\n",
    "#val_di=the dictionary created after calling function\n",
    "#fct_di_results_gridsearchcv_various_score_metrics\n",
    "def fct_creation_df_performance_classifiers_from_gridsearchcv(val_di):\n",
    "    \n",
    "    #di_rep=dict, key=id metric, vamue=[score, best model]\n",
    "    di_rep={}\n",
    "    #for each metric\n",
    "    for i in val_di:\n",
    "        #print()\n",
    "        #print(\"i\",i)\n",
    "        #print(\"val_di[i][0] gridsearch object.best_estimator_.steps: \",\\\n",
    "        #      val_di[i][0].best_estimator_.steps)\n",
    "        \n",
    "        #the best pipeline\n",
    "        best_pipeline=val_di[i][0].best_estimator_.steps[0][1]\n",
    "        \n",
    "        #if the pipeline has a scaler\n",
    "        if len(best_pipeline)>1:\n",
    "            best_model=best_pipeline[1]\n",
    "        #if the pipeline has not a scaler\n",
    "        else:\n",
    "            best_model=best_pipeline[0]\n",
    "    \n",
    "        #print(\"best_model\",best_model)\n",
    "        di_rep[i]=[val_di[i][1],best_model]\n",
    "        \n",
    "    return di_rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d42140b2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a582daf6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa05d798",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47dd6697",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c8b13d0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
